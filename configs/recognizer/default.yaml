input_dim: 80
encoder_dim: 256
num_encoder_layers: 17
num_decoder_layers: 2
num_attention_heads: 8
feed_forward_expansion_factor: 4
conv_expansion_factor: 2
input_dropout_p: 0.1
feed_forward_dropout_p: 0.1
attention_dropout_p: 0.1
conv_dropout_p: 0.1
decoder_dropout_p: 0.1
conv_kernel_size: 31
half_step_residual: True
max_length: 128
peak_lr: 1e-04
final_lr: 1e-07
init_lr_scale: 0.01
final_lr_scale: 0.05
warmup_steps: 10000
decay_steps: 80000
teacher_forcing_ratio: 1.0
cross_entropy_weight: 0.7
ctc_weight: 0.3
joint_ctc_attention: True
optimizer: adam
lr_scheduler: transformer